{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n       \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-31T14:40:48.140566Z","iopub.execute_input":"2021-07-31T14:40:48.141132Z","iopub.status.idle":"2021-07-31T14:40:49.133800Z","shell.execute_reply.started":"2021-07-31T14:40:48.141003Z","shell.execute_reply":"2021-07-31T14:40:49.132981Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt \n\nimport transformers\nimport random\n\n\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\n\nscaler = torch.cuda.amp.GradScaler() \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:40:49.135510Z","iopub.execute_input":"2021-07-31T14:40:49.135822Z","iopub.status.idle":"2021-07-31T14:40:50.646856Z","shell.execute_reply.started":"2021-07-31T14:40:49.135790Z","shell.execute_reply":"2021-07-31T14:40:50.645893Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Seed","metadata":{}},{"cell_type":"code","source":"SEED = 508\n\ndef random_seed(SEED):\n    \n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n\nrandom_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:40:50.648853Z","iopub.execute_input":"2021-07-31T14:40:50.649187Z","iopub.status.idle":"2021-07-31T14:40:50.659420Z","shell.execute_reply.started":"2021-07-31T14:40:50.649152Z","shell.execute_reply":"2021-07-31T14:40:50.658480Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:40:50.661448Z","iopub.execute_input":"2021-07-31T14:40:50.661868Z","iopub.status.idle":"2021-07-31T14:40:50.781013Z","shell.execute_reply.started":"2021-07-31T14:40:50.661828Z","shell.execute_reply":"2021-07-31T14:40:50.779764Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"          id url_legal license  \\\n0  c12129c31       NaN     NaN   \n1  85aa80a4c       NaN     NaN   \n2  b69ac6792       NaN     NaN   \n3  dd1000b26       NaN     NaN   \n4  37c1b32fb       NaN     NaN   \n\n                                             excerpt    target  standard_error  \n0  When the young people returned to the ballroom... -0.340259        0.464009  \n1  All through dinner time, Mrs. Fayre was somewh... -0.315372        0.480805  \n2  As Roger had predicted, the snow departed as q... -0.580118        0.476676  \n3  And outside before the palace a great garden w... -1.054013        0.450007  \n4  Once upon a time there were Three Bears who li...  0.247197        0.510845  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>url_legal</th>\n      <th>license</th>\n      <th>excerpt</th>\n      <th>target</th>\n      <th>standard_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c12129c31</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>When the young people returned to the ballroom...</td>\n      <td>-0.340259</td>\n      <td>0.464009</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>85aa80a4c</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n      <td>-0.315372</td>\n      <td>0.480805</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b69ac6792</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>As Roger had predicted, the snow departed as q...</td>\n      <td>-0.580118</td>\n      <td>0.476676</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dd1000b26</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>And outside before the palace a great garden w...</td>\n      <td>-1.054013</td>\n      <td>0.450007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>37c1b32fb</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Once upon a time there were Three Bears who li...</td>\n      <td>0.247197</td>\n      <td>0.510845</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.excerpt[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:40:50.782830Z","iopub.execute_input":"2021-07-31T14:40:50.783298Z","iopub.status.idle":"2021-07-31T14:40:50.793342Z","shell.execute_reply.started":"2021-07-31T14:40:50.783253Z","shell.execute_reply":"2021-07-31T14:40:50.791767Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\\nThe floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\\nAt each end of the room, on the wall, hung a beautiful bear-skin rug.\\nThese rugs were for prizes, one for the girls and one for the boys. And this was the game.\\nThe girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\\nThis would have been an easy matter, but each traveller was obliged to wear snowshoes.'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Make Train fold","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nskf=KFold(n_splits=5,shuffle=True,random_state=5)\ntrain[\"fold\"]=-1\nX=train.sample(frac=1)\nfor i,(_,val_idx) in enumerate(skf.split(X)):\n    train.loc[val_idx,\"fold\"]=i","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:40:50.795444Z","iopub.execute_input":"2021-07-31T14:40:50.795895Z","iopub.status.idle":"2021-07-31T14:40:50.813071Z","shell.execute_reply.started":"2021-07-31T14:40:50.795854Z","shell.execute_reply":"2021-07-31T14:40:50.812188Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ntest","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:40:50.814373Z","iopub.execute_input":"2021-07-31T14:40:50.814742Z","iopub.status.idle":"2021-07-31T14:40:50.835028Z","shell.execute_reply.started":"2021-07-31T14:40:50.814710Z","shell.execute_reply":"2021-07-31T14:40:50.833938Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"          id                                    url_legal       license  \\\n0  c0f722661                                          NaN           NaN   \n1  f0953f0a5                                          NaN           NaN   \n2  0df072751                                          NaN           NaN   \n3  04caf4e0c  https://en.wikipedia.org/wiki/Cell_division  CC BY-SA 3.0   \n4  0e63f8bea      https://en.wikipedia.org/wiki/Debugging  CC BY-SA 3.0   \n5  12537fe78                                          NaN           NaN   \n6  965e592c0           https://www.africanstorybook.org/#     CC BY 4.0   \n\n                                             excerpt  \n0  My hope lay in Jack's promise that he would ke...  \n1  Dotty continued to go to Mrs. Gray's every nig...  \n2  It was a bright and cheerful scene that greete...  \n3  Cell division is the process by which a parent...  \n4  Debugging is the process of finding and resolv...  \n5  To explain transitivity, let us look first at ...  \n6  Milka and John are playing in the garden. Her ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>url_legal</th>\n      <th>license</th>\n      <th>excerpt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c0f722661</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>My hope lay in Jack's promise that he would ke...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f0953f0a5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Dotty continued to go to Mrs. Gray's every nig...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0df072751</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>It was a bright and cheerful scene that greete...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>04caf4e0c</td>\n      <td>https://en.wikipedia.org/wiki/Cell_division</td>\n      <td>CC BY-SA 3.0</td>\n      <td>Cell division is the process by which a parent...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0e63f8bea</td>\n      <td>https://en.wikipedia.org/wiki/Debugging</td>\n      <td>CC BY-SA 3.0</td>\n      <td>Debugging is the process of finding and resolv...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12537fe78</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>To explain transitivity, let us look first at ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>965e592c0</td>\n      <td>https://www.africanstorybook.org/#</td>\n      <td>CC BY 4.0</td>\n      <td>Milka and John are playing in the garden. Her ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sample = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsample","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:40:50.837870Z","iopub.execute_input":"2021-07-31T14:40:50.838297Z","iopub.status.idle":"2021-07-31T14:40:50.856535Z","shell.execute_reply.started":"2021-07-31T14:40:50.838259Z","shell.execute_reply":"2021-07-31T14:40:50.855786Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"          id  target\n0  c0f722661     0.0\n1  f0953f0a5     0.0\n2  0df072751     0.0\n3  04caf4e0c     0.0\n4  0e63f8bea     0.0\n5  12537fe78     0.0\n6  965e592c0     0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c0f722661</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f0953f0a5</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0df072751</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>04caf4e0c</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0e63f8bea</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12537fe78</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>965e592c0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Tokenizer Test","metadata":{}},{"cell_type":"code","source":"# kaggle offline mode: submit is offline because internet is offline. It comes from the published dataset.\ntokenizer = transformers.BertTokenizer.from_pretrained(\"../input/bert-base-uncased\")\n\n# Please use this when using online such as local PC.\n# tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:40:50.859704Z","iopub.execute_input":"2021-07-31T14:40:50.859951Z","iopub.status.idle":"2021-07-31T14:40:51.100523Z","shell.execute_reply.started":"2021-07-31T14:40:50.859929Z","shell.execute_reply":"2021-07-31T14:40:51.099585Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test_s = train[\"excerpt\"].iloc[0]\ntest_s","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:40:51.103001Z","iopub.execute_input":"2021-07-31T14:40:51.103588Z","iopub.status.idle":"2021-07-31T14:40:51.110312Z","shell.execute_reply.started":"2021-07-31T14:40:51.103548Z","shell.execute_reply":"2021-07-31T14:40:51.109307Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\\nThe floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\\nAt each end of the room, on the wall, hung a beautiful bear-skin rug.\\nThese rugs were for prizes, one for the girls and one for the boys. And this was the game.\\nThe girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\\nThis would have been an easy matter, but each traveller was obliged to wear snowshoes.'"},"metadata":{}}]},{"cell_type":"code","source":"result1=tokenizer.encode_plus(test_s)\nresult1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:40:51.111844Z","iopub.execute_input":"2021-07-31T14:40:51.112297Z","iopub.status.idle":"2021-07-31T14:40:51.129841Z","shell.execute_reply.started":"2021-07-31T14:40:51.112255Z","shell.execute_reply":"2021-07-31T14:40:51.128669Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 2043, 1996, 2402, 2111, 2513, 2000, 1996, 14307, 1010, 2009, 3591, 1037, 27873, 2904, 3311, 1012, 2612, 1997, 2019, 4592, 3496, 1010, 2009, 2001, 1037, 3467, 5957, 1012, 1996, 2723, 2001, 3139, 2007, 4586, 1011, 2317, 10683, 1010, 2025, 4201, 2006, 15299, 1010, 2021, 19379, 21132, 2058, 18548, 1998, 2940, 25384, 1010, 2066, 1037, 2613, 4586, 2492, 1012, 1996, 3365, 9486, 1998, 16899, 2015, 2008, 2018, 7429, 1996, 2282, 1010, 2020, 9898, 2098, 2007, 13724, 1998, 25259, 2007, 25252, 1997, 6557, 1010, 2066, 4586, 1012, 2036, 6323, 6497, 2018, 2042, 8217, 11867, 6657, 19859, 2006, 2068, 1010, 1998, 20332, 6121, 24582, 20921, 5112, 2013, 1996, 5628, 1012, 2012, 2169, 2203, 1997, 1996, 2282, 1010, 2006, 1996, 2813, 1010, 5112, 1037, 3376, 4562, 1011, 3096, 20452, 1012, 2122, 20452, 2015, 2020, 2005, 11580, 1010, 2028, 2005, 1996, 3057, 1998, 2028, 2005, 1996, 3337, 1012, 1998, 2023, 2001, 1996, 2208, 1012, 1996, 3057, 2020, 5935, 2012, 2028, 2203, 1997, 1996, 2282, 1998, 1996, 3337, 2012, 1996, 2060, 1010, 1998, 2028, 2203, 2001, 2170, 1996, 2167, 6536, 1010, 1998, 1996, 2060, 1996, 2148, 6536, 1012, 2169, 2447, 2001, 2445, 1037, 2235, 5210, 2029, 2027, 2020, 2000, 3269, 2006, 4285, 1996, 6536, 1012, 2023, 2052, 2031, 2042, 2019, 3733, 3043, 1010, 2021, 2169, 21916, 2001, 14723, 2000, 4929, 4586, 22231, 2229, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode(result1[\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:40:51.131453Z","iopub.execute_input":"2021-07-31T14:40:51.131833Z","iopub.status.idle":"2021-07-31T14:40:55.464692Z","shell.execute_reply.started":"2021-07-31T14:40:51.131794Z","shell.execute_reply":"2021-07-31T14:40:55.463914Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'[CLS] when the young people returned to the ballroom, it presented a decidedly changed appearance. instead of an interior scene, it was a winter landscape. the floor was covered with snow - white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. the numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches. at each end of the room, on the wall, hung a beautiful bear - skin rug. these rugs were for prizes, one for the girls and one for the boys. and this was the game. the girls were gathered at one end of the room and the boys at the other, and one end was called the north pole, and the other the south pole. each player was given a small flag which they were to plant on reaching the pole. this would have been an easy matter, but each traveller was obliged to wear snowshoes. [SEP]'"},"metadata":{}}]},{"cell_type":"code","source":"sen_length = []\n\nfor sentence in tqdm(train[\"excerpt\"]):\n\n    token_words = tokenizer.encode_plus(sentence)[\"input_ids\"]\n    sen_length.append(len(token_words))\n\nprint('maxlenth of all sentences are  ', max(sen_length))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:40:55.466214Z","iopub.execute_input":"2021-07-31T14:40:55.466562Z","iopub.status.idle":"2021-07-31T14:41:11.418210Z","shell.execute_reply.started":"2021-07-31T14:40:55.466527Z","shell.execute_reply":"2021-07-31T14:41:11.417409Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"100%|██████████| 2834/2834 [00:15<00:00, 177.82it/s]","output_type":"stream"},{"name":"stdout","text":"maxlenth of all sentences are   314\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"test_s","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.419281Z","iopub.execute_input":"2021-07-31T14:41:11.419560Z","iopub.status.idle":"2021-07-31T14:41:11.428279Z","shell.execute_reply.started":"2021-07-31T14:41:11.419533Z","shell.execute_reply":"2021-07-31T14:41:11.427238Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\\nThe floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\\nAt each end of the room, on the wall, hung a beautiful bear-skin rug.\\nThese rugs were for prizes, one for the girls and one for the boys. And this was the game.\\nThe girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\\nThis would have been an easy matter, but each traveller was obliged to wear snowshoes.'"},"metadata":{}}]},{"cell_type":"code","source":"result2 = tokenizer.encode_plus(\n    test_s,\n    add_special_tokens = True, # Whether to insert [CLS], [SEP]\n    max_length = 500,#314, # Align the number of words using padding and transcription\n    pad_to_max_length = True, # Put [PAD] in the blank area\n    \n    truncation = True # Cutout function. For example, max_length10 is a function that makes only the first 10 characters. I got an alert if I didn't put it in, so I'll put it in\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.429688Z","iopub.execute_input":"2021-07-31T14:41:11.430137Z","iopub.status.idle":"2021-07-31T14:41:11.441488Z","shell.execute_reply.started":"2021-07-31T14:41:11.430103Z","shell.execute_reply":"2021-07-31T14:41:11.440733Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"result2","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.442836Z","iopub.execute_input":"2021-07-31T14:41:11.443308Z","iopub.status.idle":"2021-07-31T14:41:11.457774Z","shell.execute_reply.started":"2021-07-31T14:41:11.443273Z","shell.execute_reply":"2021-07-31T14:41:11.456985Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [101, 2043, 1996, 2402, 2111, 2513, 2000, 1996, 14307, 1010, 2009, 3591, 1037, 27873, 2904, 3311, 1012, 2612, 1997, 2019, 4592, 3496, 1010, 2009, 2001, 1037, 3467, 5957, 1012, 1996, 2723, 2001, 3139, 2007, 4586, 1011, 2317, 10683, 1010, 2025, 4201, 2006, 15299, 1010, 2021, 19379, 21132, 2058, 18548, 1998, 2940, 25384, 1010, 2066, 1037, 2613, 4586, 2492, 1012, 1996, 3365, 9486, 1998, 16899, 2015, 2008, 2018, 7429, 1996, 2282, 1010, 2020, 9898, 2098, 2007, 13724, 1998, 25259, 2007, 25252, 1997, 6557, 1010, 2066, 4586, 1012, 2036, 6323, 6497, 2018, 2042, 8217, 11867, 6657, 19859, 2006, 2068, 1010, 1998, 20332, 6121, 24582, 20921, 5112, 2013, 1996, 5628, 1012, 2012, 2169, 2203, 1997, 1996, 2282, 1010, 2006, 1996, 2813, 1010, 5112, 1037, 3376, 4562, 1011, 3096, 20452, 1012, 2122, 20452, 2015, 2020, 2005, 11580, 1010, 2028, 2005, 1996, 3057, 1998, 2028, 2005, 1996, 3337, 1012, 1998, 2023, 2001, 1996, 2208, 1012, 1996, 3057, 2020, 5935, 2012, 2028, 2203, 1997, 1996, 2282, 1998, 1996, 3337, 2012, 1996, 2060, 1010, 1998, 2028, 2203, 2001, 2170, 1996, 2167, 6536, 1010, 1998, 1996, 2060, 1996, 2148, 6536, 1012, 2169, 2447, 2001, 2445, 1037, 2235, 5210, 2029, 2027, 2020, 2000, 3269, 2006, 4285, 1996, 6536, 1012, 2023, 2052, 2031, 2042, 2019, 3733, 3043, 1010, 2021, 2169, 21916, 2001, 14723, 2000, 4929, 4586, 22231, 2229, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode(result2[\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.459169Z","iopub.execute_input":"2021-07-31T14:41:11.459571Z","iopub.status.idle":"2021-07-31T14:41:11.468560Z","shell.execute_reply.started":"2021-07-31T14:41:11.459539Z","shell.execute_reply":"2021-07-31T14:41:11.467570Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'[CLS] when the young people returned to the ballroom, it presented a decidedly changed appearance. instead of an interior scene, it was a winter landscape. the floor was covered with snow - white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. the numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches. at each end of the room, on the wall, hung a beautiful bear - skin rug. these rugs were for prizes, one for the girls and one for the boys. and this was the game. the girls were gathered at one end of the room and the boys at the other, and one end was called the north pole, and the other the south pole. each player was given a small flag which they were to plant on reaching the pole. this would have been an easy matter, but each traveller was obliged to wear snowshoes. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"},"metadata":{}}]},{"cell_type":"code","source":"train = train.sort_values(\"target\").reset_index(drop=True)\ntrain","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.470005Z","iopub.execute_input":"2021-07-31T14:41:11.470496Z","iopub.status.idle":"2021-07-31T14:41:11.494883Z","shell.execute_reply.started":"2021-07-31T14:41:11.470396Z","shell.execute_reply":"2021-07-31T14:41:11.493893Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"             id                                          url_legal  \\\n0     4626100d8                                                NaN   \n1     493b80aa7                                                NaN   \n2     fe44cbd14                                                NaN   \n3     284eaa5ad                                                NaN   \n4     9e9eacb49                                                NaN   \n...         ...                                                ...   \n2829  016913371                  https://www.africanstorybook.org/   \n2830  7a1d484be                  https://www.africanstorybook.org/   \n2831  8f35441e3                 https://www.africanstorybook.org/#   \n2832  849971671                  https://www.africanstorybook.org/   \n2833  25ca8f498  https://sites.ehe.osu.edu/beyondpenguins/files...   \n\n           license                                            excerpt  \\\n0              NaN  The commutator is peculiar, consisting of only...   \n1              NaN  The Dunwich horror itself came between Lammas ...   \n2              NaN  The iron cylinder weighs 23 kilogrammes; but, ...   \n3              NaN  As to surface-slope its measurement—from nearl...   \n4              NaN  The tree is dioecious, bearing male catkins on...   \n...            ...                                                ...   \n2829     CC BY 4.0  Grandma's garden was wonderful. It was full of...   \n2830     CC BY 4.0  More people came to the bus stop just before 9...   \n2831     CC BY 4.0  Every day, Emeka's father took him to school i...   \n2832     CC BY 4.0  For her last birthday, Sisanda had a special t...   \n2833  CC BY-SA 3.0  When you think of dinosaurs and where they liv...   \n\n        target  standard_error  fold  \n0    -3.676268        0.623621     0  \n1    -3.668360        0.571404     1  \n2    -3.642892        0.644398     1  \n3    -3.639936        0.603819     3  \n4    -3.636834        0.606822     0  \n...        ...             ...   ...  \n2829  1.467665        0.599600     3  \n2830  1.541672        0.606997     2  \n2831  1.583847        0.624776     4  \n2832  1.597870        0.596349     4  \n2833  1.711390        0.646900     2  \n\n[2834 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>url_legal</th>\n      <th>license</th>\n      <th>excerpt</th>\n      <th>target</th>\n      <th>standard_error</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4626100d8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The commutator is peculiar, consisting of only...</td>\n      <td>-3.676268</td>\n      <td>0.623621</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>493b80aa7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The Dunwich horror itself came between Lammas ...</td>\n      <td>-3.668360</td>\n      <td>0.571404</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fe44cbd14</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The iron cylinder weighs 23 kilogrammes; but, ...</td>\n      <td>-3.642892</td>\n      <td>0.644398</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>284eaa5ad</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>As to surface-slope its measurement—from nearl...</td>\n      <td>-3.639936</td>\n      <td>0.603819</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9e9eacb49</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The tree is dioecious, bearing male catkins on...</td>\n      <td>-3.636834</td>\n      <td>0.606822</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2829</th>\n      <td>016913371</td>\n      <td>https://www.africanstorybook.org/</td>\n      <td>CC BY 4.0</td>\n      <td>Grandma's garden was wonderful. It was full of...</td>\n      <td>1.467665</td>\n      <td>0.599600</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2830</th>\n      <td>7a1d484be</td>\n      <td>https://www.africanstorybook.org/</td>\n      <td>CC BY 4.0</td>\n      <td>More people came to the bus stop just before 9...</td>\n      <td>1.541672</td>\n      <td>0.606997</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2831</th>\n      <td>8f35441e3</td>\n      <td>https://www.africanstorybook.org/#</td>\n      <td>CC BY 4.0</td>\n      <td>Every day, Emeka's father took him to school i...</td>\n      <td>1.583847</td>\n      <td>0.624776</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2832</th>\n      <td>849971671</td>\n      <td>https://www.africanstorybook.org/</td>\n      <td>CC BY 4.0</td>\n      <td>For her last birthday, Sisanda had a special t...</td>\n      <td>1.597870</td>\n      <td>0.596349</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2833</th>\n      <td>25ca8f498</td>\n      <td>https://sites.ehe.osu.edu/beyondpenguins/files...</td>\n      <td>CC BY-SA 3.0</td>\n      <td>When you think of dinosaurs and where they liv...</td>\n      <td>1.711390</td>\n      <td>0.646900</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>2834 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# p_train = train[train[\"kfold\"]!=0].reset_index(drop=True)\n# p_valid = train[train[\"kfold\"]==0].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.495909Z","iopub.execute_input":"2021-07-31T14:41:11.496397Z","iopub.status.idle":"2021-07-31T14:41:11.500259Z","shell.execute_reply.started":"2021-07-31T14:41:11.496361Z","shell.execute_reply":"2021-07-31T14:41:11.499023Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class BERTDataset(Dataset):\n    \n    def __init__(self,sentences,targets,max_len=500):\n        self.sentences = sentences\n        self.targets = targets\n        self.max_len=max_len\n        \n    def __len__(self):\n        return len(self.sentences)\n    \n    def __getitem__(self,idx):\n        sentence = self.sentences[idx]\n        bert_sens = tokenizer.encode_plus(\n                                sentence,\n                                add_special_tokens = True, \n                                max_length =self.max_len,\n                                pad_to_max_length = True, \n                                return_attention_mask = True)\n\n        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)\n        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(bert_sens['token_type_ids'], dtype=torch.long)\n        target = torch.tensor(self.targets[idx],dtype=torch.float)\n        \n        return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                'targets': target\n            }","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.501746Z","iopub.execute_input":"2021-07-31T14:41:11.502120Z","iopub.status.idle":"2021-07-31T14:41:11.514386Z","shell.execute_reply.started":"2021-07-31T14:41:11.502072Z","shell.execute_reply":"2021-07-31T14:41:11.513321Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{}},{"cell_type":"code","source":"class CFG:\n    frac=1.0#0.1 \n    train=True#False#\n    pretrained=True #False#\n    model= \"BERT\"#\"R34\"#\"E4\"#\n    train_bs=12\n    valid_bs=8#12\n    test_bs=2\n    epochs=30\n    fold=0\n    classes=1\n    seed= 719\n    lr=2e-4\n    es=4\n","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.515897Z","iopub.execute_input":"2021-07-31T14:41:11.516296Z","iopub.status.idle":"2021-07-31T14:41:11.523163Z","shell.execute_reply.started":"2021-07-31T14:41:11.516258Z","shell.execute_reply":"2021-07-31T14:41:11.521905Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Engine","metadata":{}},{"cell_type":"code","source":"# class Engine:\n#     def __init__(self,model,optimizer,scheduler):\n#         self.model=model\n#         self.optimizer=optimizer\n#         self.scheduler=scheduler\n        \n#     def get_accuracy(self,labels,preds):\n#         total=labels.shape[0]\n#         if preds.shape[1]==1:\n#             l=labels\n#             p=np.uint8(preds>0.5)\n#         else:\n#             p=preds.argmax(1).reshape(-1,1)\n#             l=labels.reshape(-1,1)\n#         return np.uint8(l==p).sum()/total\n    \n#     def loss_fn(self,outputs,targets):\n#         return nn.MSELoss()(outputs,targets)  \n    \n#     def train(self,data_loader):\n#         l_preds=[]\n#         l_labels=[]\n#         self.model.train()\n#         final_loss=0\n#         for data in data_loader:\n#             self.optimizer.zero_grad()\n#             print(data.shape)\n            \n#             ids=data[\"ids\"].to(device,non_blocking=True)\n#             mask=data[\"mask\"].to(device,non_blocking=True)\n#             tokentype=data[\"token_type_ids\"].to(device,non_blocking=True)\n\n#             outputs=self.model(ids,mask)\n#             outputs=outputs[\"logits\"].squeeze(-1)\n#             targets=data[\"targets\"].to(device,non_blocking=True)\n#             loss=self.loss_fn(outputs,targets)\n                \n#             loss.backward()\n#             self.optimizer.step()\n            \n#             ##accuracy\n#             final_loss+=loss.item()\n#             l_preds.append(outputs.cpu().detach().numpy().reshape(-1,1))\n#             l_labels.append(targets.cpu().detach().numpy().reshape(-1,1))            \n#             l_preds=np.concatenate(l_preds)\n#             l_labels=np.concatenate(l_labels)\n            \n#         return np.mean(final_loss), l_preds, l_labels\n    \n#     def validate(self,data_loader):\n#         device=self.device\n#         preds_for_acc=[]\n#         labels_for_acc=[]\n#         self.model.eval()\n#         final_loss=0\n#         for data in data_loader:\n#             ids=data[\"ids\"].to(device,non_blocking=True)\n#             mask=data[\"mask\"].to(device,non_blocking=True)\n#             tokentype=data[\"token_type_ids\"].to(device,non_blocking=True)\n\n#             with torch.no_grad():\n#                 outputs=self.model(ids,mask)\n#                 outputs=outputs[\"logits\"].squeeze(-1)\n#                 targets=data[\"targets\"].to(device,non_blocking=True)\n\n#                 loss=self.loss_fn(outputs,targets)\n#                 final_loss+=loss.item()\n            \n#             ##accuracy\n#             labels=targets.cpu().numpy().reshape(-1,1)\n#             preds=outputs.cpu().detach().numpy()\n#             if len(labels_for_acc)==0:\n#                 labels_for_acc=labels\n#                 preds_for_acc=preds\n#             else:\n#                 labels_for_acc=np.vstack((labels_for_acc,labels))\n#                 preds_for_acc=np.vstack((preds_for_acc,preds))\n#         accuracy=self.get_accuracy(labels_for_acc,preds_for_acc)\n#         return final_loss/len(data_loader),accuracy,labels_for_acc,preds_for_acc\n\n#     def predict(self,data_loader):\n#         self.model.eval()\n#         final_predictions=[]\n#         for data in data_loader:\n#             inputs=data[\"img\"].to(self.device)\n#             outputs=self.model(inputs)\n#             outputs=outputs.cpu()\n#             final_predictions.append(outputs.detach().numpy())\n#         return final_predictions","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.524836Z","iopub.execute_input":"2021-07-31T14:41:11.525534Z","iopub.status.idle":"2021-07-31T14:41:11.533943Z","shell.execute_reply.started":"2021-07-31T14:41:11.525497Z","shell.execute_reply":"2021-07-31T14:41:11.532989Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.539079Z","iopub.execute_input":"2021-07-31T14:41:11.539338Z","iopub.status.idle":"2021-07-31T14:41:11.548454Z","shell.execute_reply.started":"2021-07-31T14:41:11.539310Z","shell.execute_reply":"2021-07-31T14:41:11.547532Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"class Engine:\n    def __init__(self,model,optimizer,device,classes):\n        self.model=model\n        self.optimizer=optimizer\n        self.device=device\n        self.classes=classes\n        \n    def get_accuracy(self,labels,preds):\n        total=labels.shape[0]\n        if preds.shape[1]==1:\n            l=labels\n            p=np.uint8(preds>0.5)\n        else:\n            p=preds.argmax(1).reshape(-1,1)\n            l=labels.reshape(-1,1)\n        return np.uint8(l==p).sum()/total\n    \n    def loss_fn(self,outputs,targets):\n        return nn.MSELoss()(outputs,targets)  \n    \n    def train(self,data_loader):\n        preds_for_acc=[]\n        labels_for_acc=[]\n        self.model.train()\n        final_loss=0\n        for data in data_loader:\n            self.optimizer.zero_grad()\n            ids=data[\"ids\"].to(device,non_blocking=True)\n            mask=data[\"mask\"].to(device,non_blocking=True)\n            tokentype=data[\"token_type_ids\"].to(device,non_blocking=True)\n\n            outputs=self.model(ids,mask)\n            outputs=outputs[\"logits\"].squeeze(-1)\n            targets=data[\"targets\"].to(device,non_blocking=True)\n            \n            loss=self.loss_fn(outputs,targets)\n            \n            loss.backward()\n            self.optimizer.step()\n            final_loss+=loss.item()\n            \n            ##accuracy\n            labels=targets.cpu().numpy().reshape(-1,1)            \n            preds=outputs.cpu().detach().numpy().reshape(-1,1)\n            if len(labels_for_acc)==0:\n                labels_for_acc=labels\n                preds_for_acc=preds\n            else:\n                labels_for_acc=np.vstack((labels_for_acc,labels))\n                preds_for_acc=np.vstack((preds_for_acc,preds))\n        return final_loss/len(data_loader),labels_for_acc,preds_for_acc\n    \n    def validate(self,data_loader):\n        preds_for_acc=[]\n        labels_for_acc=[]\n        self.model.eval()\n        final_loss=0\n        for data in data_loader:\n            ids=data[\"ids\"].to(device,non_blocking=True)\n            mask=data[\"mask\"].to(device,non_blocking=True)\n            tokentype=data[\"token_type_ids\"].to(device,non_blocking=True)\n\n            with torch.no_grad():\n                outputs=self.model(ids,mask)\n                outputs=outputs[\"logits\"].squeeze(-1)\n                targets=data[\"targets\"].to(device,non_blocking=True)\n\n                loss=self.loss_fn(outputs,targets)\n                final_loss+=loss.item()\n            \n            ##accuracy\n            labels=targets.cpu().numpy().reshape(-1,1)\n            preds=outputs.cpu().detach().numpy().reshape(-1,1)\n            if len(labels_for_acc)==0:\n                labels_for_acc=labels\n                preds_for_acc=preds\n            else:\n                labels_for_acc=np.vstack((labels_for_acc,labels))\n                preds_for_acc=np.vstack((preds_for_acc,preds))\n        accuracy=self.get_accuracy(labels_for_acc,preds_for_acc)\n        return final_loss/len(data_loader),labels_for_acc,preds_for_acc\n\n    def predict(self,data_loader):\n        self.model.eval()\n        final_predictions=[]\n        for data in data_loader:\n            ids=data[\"ids\"].to(device,non_blocking=True)\n            mask=data[\"mask\"].to(device,non_blocking=True)\n            tokentype=data[\"token_type_ids\"].to(device,non_blocking=True)\n            outputs=self.model(ids,mask)\n            outputs=outputs[\"logits\"].squeeze(-1)\n            outputs=outputs.cpu()\n            final_predictions.append(outputs.detach().numpy().reshape(-1,1))\n        return final_predictions","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.551209Z","iopub.execute_input":"2021-07-31T14:41:11.552003Z","iopub.status.idle":"2021-07-31T14:41:11.574530Z","shell.execute_reply.started":"2021-07-31T14:41:11.551931Z","shell.execute_reply":"2021-07-31T14:41:11.573609Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# train","metadata":{}},{"cell_type":"code","source":"import datetime\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport gc\n\ndef my_log(str):\n    tm=datetime.datetime.now().strftime(\"%H:%M:%S\")\n    print(f\"{tm}, {str}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.575884Z","iopub.execute_input":"2021-07-31T14:41:11.576328Z","iopub.status.idle":"2021-07-31T14:41:11.593709Z","shell.execute_reply.started":"2021-07-31T14:41:11.576290Z","shell.execute_reply":"2021-07-31T14:41:11.592831Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def train_model(fold):\n    df_train=train[train.fold!=fold].reset_index(drop=True)\n    df_valid=train[train.fold==fold].reset_index(drop=True)\n    \n    df_train=df_train.sample(frac=CFG.frac,random_state=999)\n    df_valid=df_valid.sample(frac=CFG.frac,random_state=999)\n\n    train_dataset=BERTDataset(df_train.excerpt.values,df_train.target.values)\n    valid_dataset=BERTDataset(df_valid.excerpt.values,df_valid.target.values)\n    \n    train_loader=DataLoader(train_dataset,batch_size=CFG.train_bs,shuffle=True,\\\n                            num_workers=4,pin_memory=True)\n    valid_loader=DataLoader(valid_dataset,batch_size=CFG.valid_bs,shuffle=False,\\\n                            num_workers=4,pin_memory=True)\n\n#     model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\\\n#                                                                        num_labels=1)\n    model = transformers.BertForSequenceClassification.from_pretrained(\\\n                                \"../input/bert-base-uncased\",num_labels=1)\n    model.to(device)\n    \n    optimizer=AdamW(model.parameters(),CFG.lr,betas=(0.9,0.999),weight_decay=1e-2)    \n    train_steps=int(len(df_train)/CFG.train_bs*CFG.epochs)\n    num_steps=int(train_steps*.1)\n    scheduler=get_linear_schedule_with_warmup(optimizer,num_steps,train_steps)\n    \n    engine=Engine(model,optimizer,device,1)\n    \n    best_loss=np.inf\n    early_stopping_cnt=0\n    \n    for epoch in range(CFG.epochs):\n        t_loss,t_labels,t_preds=engine.train(train_loader)\n        v_loss,v_labels,v_preds=engine.validate(valid_loader)\n        scheduler.step(v_loss)\n        \n        my_log(f\"fold={fold},epoch={epoch},t_loss={t_loss:.4f},v_loss={v_loss:.4f}\")\n        if v_loss<best_loss:\n            best_loss=v_loss\n            torch.save(model.state_dict(),f\"model_{CFG.model}_fold_{fold}.bin\")\n            early_stopping_cnt=0\n        else:\n            early_stopping_cnt+=1\n        if early_stopping_cnt>=CFG.es:\n            break\n            \n            \n    del train_dataset,valid_dataset,train_loader,valid_loader,model,optimizer,scheduler\n    _ = gc.collect()    ","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.595262Z","iopub.execute_input":"2021-07-31T14:41:11.595729Z","iopub.status.idle":"2021-07-31T14:41:11.608666Z","shell.execute_reply.started":"2021-07-31T14:41:11.595692Z","shell.execute_reply":"2021-07-31T14:41:11.607481Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_model(fold=0)\ntrain_model(fold=1)\ntrain_model(fold=2)\ntrain_model(fold=3)\ntrain_model(fold=4)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:41:11.610258Z","iopub.execute_input":"2021-07-31T14:41:11.610671Z","iopub.status.idle":"2021-07-31T14:42:45.444778Z","shell.execute_reply.started":"2021-07-31T14:41:11.610634Z","shell.execute_reply":"2021-07-31T14:42:45.443901Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"14:41:40, fold=0,epoch=0,t_loss=2.1271,v_loss=2.2430\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"14:41:56, fold=0,epoch=1,t_loss=0.9972,v_loss=0.6122\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"14:42:12, fold=0,epoch=2,t_loss=0.8198,v_loss=0.5288\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"14:42:27, fold=0,epoch=3,t_loss=0.6452,v_loss=0.3949\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"14:42:43, fold=0,epoch=4,t_loss=0.4771,v_loss=0.3253\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\ndef predict(fold):\n    test_df=test\n    test_dataset=BERTDataset(test_df.excerpt.values,np.zeros(test_df.shape[0]))\n    test_loader=DataLoader(test_dataset,batch_size=CFG.test_bs,shuffle=False,\\\n                            num_workers=4,pin_memory=True)\n\n    model = transformers.BertForSequenceClassification.from_pretrained(\\\n                                \"../input/bert-base-uncased\",num_labels=1)\n    model_save_path=f\"model_{CFG.model}_fold_{fold}.bin\"\n    model.load_state_dict(torch.load(model_save_path))\n    model=model.to(device)\n\n    engine=Engine(model,None,device,classes=1)\n    preds=engine.predict(test_loader)\n    preds=np.vstack(preds)\n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:42:45.446380Z","iopub.execute_input":"2021-07-31T14:42:45.446693Z","iopub.status.idle":"2021-07-31T14:42:45.453511Z","shell.execute_reply.started":"2021-07-31T14:42:45.446656Z","shell.execute_reply":"2021-07-31T14:42:45.452665Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"p=predict(0)\np","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:42:45.454808Z","iopub.execute_input":"2021-07-31T14:42:45.455372Z","iopub.status.idle":"2021-07-31T14:42:47.847189Z","shell.execute_reply.started":"2021-07-31T14:42:45.455336Z","shell.execute_reply":"2021-07-31T14:42:47.845993Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ../input/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at ../input/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"array([[-0.37524784],\n       [-0.3924919 ],\n       [-0.5704994 ],\n       [-1.5668049 ],\n       [-1.3243016 ],\n       [-0.2674231 ],\n       [-0.27797174]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"sample[\"target\"]=p.reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:42:47.867338Z","iopub.execute_input":"2021-07-31T14:42:47.867666Z","iopub.status.idle":"2021-07-31T14:42:48.007498Z","shell.execute_reply.started":"2021-07-31T14:42:47.867633Z","shell.execute_reply":"2021-07-31T14:42:48.006580Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"sample.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:42:48.009057Z","iopub.execute_input":"2021-07-31T14:42:48.009430Z","iopub.status.idle":"2021-07-31T14:42:48.020256Z","shell.execute_reply.started":"2021-07-31T14:42:48.009394Z","shell.execute_reply":"2021-07-31T14:42:48.019450Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"sample","metadata":{"execution":{"iopub.status.busy":"2021-07-31T14:42:48.021734Z","iopub.execute_input":"2021-07-31T14:42:48.022490Z","iopub.status.idle":"2021-07-31T14:42:48.032384Z","shell.execute_reply.started":"2021-07-31T14:42:48.022450Z","shell.execute_reply":"2021-07-31T14:42:48.031313Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"          id    target\n0  c0f722661 -0.375248\n1  f0953f0a5 -0.392492\n2  0df072751 -0.570499\n3  04caf4e0c -1.566805\n4  0e63f8bea -1.324302\n5  12537fe78 -0.267423\n6  965e592c0 -0.277972","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c0f722661</td>\n      <td>-0.375248</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f0953f0a5</td>\n      <td>-0.392492</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0df072751</td>\n      <td>-0.570499</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>04caf4e0c</td>\n      <td>-1.566805</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0e63f8bea</td>\n      <td>-1.324302</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12537fe78</td>\n      <td>-0.267423</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>965e592c0</td>\n      <td>-0.277972</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}