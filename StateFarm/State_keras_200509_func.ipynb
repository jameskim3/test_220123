{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 224\n",
    "img_cols = 224\n",
    "color_type = 1\n",
    "batch_size=16\n",
    "epochs=300\n",
    "cache = 'e:/kaggle_imgs/cache/StateFarm'\n",
    "img_path=\"E:/kaggle_imgs/Statefarm/Data/imgs\"\n",
    "train_pickle=img_path+\"/train_data2.pickle\"\n",
    "test_pickle=img_path+\"/test_data2.pickle\"\n",
    "subject=\"Digit_model\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "if not os.path.exists(cache):\n",
    "    #shutil.rmtree(cache)\n",
    "    os.mkdir(cache)\n",
    "saved_path=\"{}/saved_models\".format(cache)\n",
    "if not os.path.exists(saved_path):\n",
    "    os.makedirs(saved_path)\n",
    "file_path='{}/State_keras_200508_new_func.hdf5'.format(saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2016)\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import math\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image as IM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import log_loss\n",
    "#from scipy.misc import imread, imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_data(data, path):\n",
    "    file = open(path, 'wb')\n",
    "    pickle.dump(data, file)\n",
    "    file.close()\n",
    "def restore_data(path):\n",
    "    file = open(path, 'rb')\n",
    "    data = pickle.load(file)\n",
    "    return data\n",
    "def split_validation_set(train, target, test_size):\n",
    "    random_state = 51\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    return gray\n",
    "\n",
    "def get_image(path, img_rows, img_cols, color_type):\n",
    "    img = IM.open(path)\n",
    "    img = img.resize((img_rows,img_cols))\n",
    "    img = np.array(img)\n",
    "    img = rgb2gray(img)\n",
    "    return img\n",
    "\n",
    "def get_im_cv2(path, img_rows, img_cols, color_type=1):\n",
    "    # Load as grayscale\n",
    "    if color_type == 1:\n",
    "        img = cv2.imread(path, 0)\n",
    "    elif color_type == 3:\n",
    "        img = cv2.imread(path)\n",
    "    # Reduce size\n",
    "    resized = cv2.resize(img, (img_cols, img_rows))\n",
    "    return resized\n",
    "\n",
    "def load_train(img_rows, img_cols, color_type=1):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "  \n",
    "    print('Read train images')\n",
    "    for j in range(10):\n",
    "        path = os.path.join(img_path, 'train', 'c' + str(j), '*.jpg')\n",
    "        print(path)\n",
    "        files = glob.glob(path)\n",
    "        for (i,fl) in enumerate(files):\n",
    "            flbase = os.path.basename(fl)\n",
    "            img = get_image(fl, img_rows, img_cols, color_type)\n",
    "            X_train.append(img)\n",
    "            y_train.append(j)\n",
    "    return X_train, y_train\n",
    "\n",
    "def read_and_normalize_train_data(img_rows, img_cols, color_type=1):\n",
    "    if not os.path.isfile(train_pickle):\n",
    "        print('load train from fiels!')\n",
    "        train_data, train_target = load_train(img_rows, img_cols, color_type)\n",
    "        train_data = np.array(train_data, dtype=np.uint8)\n",
    "        train_target = np.array(train_target, dtype=np.uint8)\n",
    "        train_data = train_data.reshape(train_data.shape[0], img_rows, img_cols, color_type)\n",
    "        train_target = np_utils.to_categorical(train_target, 10)\n",
    "        X_train, X_valid, y_train, y_valid=split_validation_set(train_data,train_target,0.2)\n",
    "        cache_data((X_train, X_valid, y_train, y_valid), train_pickle)\n",
    "    else:\n",
    "        print('Restore train from cache!')\n",
    "        (X_train, X_valid, y_train, y_valid) = restore_data(train_pickle)\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_train = X_train/255\n",
    "    X_valid = X_valid.astype('float32')\n",
    "    X_valid = X_valid/255\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(img_rows, img_cols, color_type=1):\n",
    "    print('Read test images')\n",
    "    path = os.path.join(img_path, 'test', '*.jpg')\n",
    "    files = glob.glob(path)\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    total = 0\n",
    "    thr = math.floor(len(files)/10)\n",
    "    for (i,fl) in enumerate(files):\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_image(fl, img_rows, img_cols, color_type)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase)\n",
    "        if i%thr == 0:\n",
    "            print('Read {} images from {}'.format(i, len(files)))\n",
    "\n",
    "    return X_test, X_test_id\n",
    "\n",
    "def read_and_normalize_test_data(img_rows, img_cols, color_type=1):\n",
    "    if not os.path.isfile(test_pickle):\n",
    "        test_data, test_id = load_test(img_rows, img_cols, color_type)\n",
    "        test_data = np.array(test_data, dtype=np.uint8)\n",
    "        test_data = test_data.reshape(test_data.shape[0], img_rows, img_cols, color_type)\n",
    "        cache_data((test_data, test_id), test_pickle)\n",
    "    else:\n",
    "        print('Restore test from cache!')\n",
    "        (test_data, test_id) = restore_data(test_pickle)\n",
    "\n",
    "    test_data = test_data.astype('float32')\n",
    "    test_data =test_data/ 255\n",
    "    return test_data, test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_img(img):\n",
    "    plt.figure()\n",
    "    plt.imshow(np.array(img.reshape(img_rows,img_cols)))\n",
    "def get_callback():\n",
    "    checkpoint = ModelCheckpoint(filepath=file_path, \n",
    "                               monitor='val_loss', mode='min',\n",
    "                               verbose=1, save_best_only=True)\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=7)\n",
    "    learning_rate_reduction=ReduceLROnPlateau(monitor=\"val_acc\",\n",
    "                                          patience=3,\n",
    "                                          verbose=1,\n",
    "                                          factor=0.5,\n",
    "                                          min_lr=0.00001)\n",
    "    return [checkpoint, es,learning_rate_reduction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                     activation ='relu', input_shape = (img_rows,img_cols,color_type)))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation = \"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation = \"softmax\"))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ImgGen(X_train, X_valid, y_train, y_valid):\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "    \n",
    "    datagen.fit(X_train)\n",
    "    training_generator = datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "    validation_data = (X_valid,y_valid)\n",
    "    return training_generator,validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restore train from cache!\n",
      "Restore test from cache!\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "X_train, X_valid, y_train, y_valid = read_and_normalize_train_data(img_rows, img_cols, color_type)\n",
    "test_data, test_id = read_and_normalize_test_data(img_rows, img_cols, color_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 32)      832       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 32)      25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                6422560   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 6,504,778\n",
      "Trainable params: 6,504,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Start Single Run\n",
      "Split train:  17939 17939\n",
      "Split valid:  4485 4485\n",
      "Epoch 1/300\n",
      "  86/1121 [=>............................] - ETA: 4:40 - loss: 2.4356 - accuracy: 0.1076 ETA: 4:50 -"
     ]
    }
   ],
   "source": [
    "#callbacks\n",
    "callbacks=get_callback()\n",
    "\n",
    "#get model\n",
    "model = get_model()\n",
    "training_generator,validation_data=get_ImgGen(X_train, X_valid, y_train, y_valid)\n",
    "\n",
    "\n",
    "print('Start Single Run')\n",
    "print('Split train: ', len(X_train), len(y_train))\n",
    "print('Split valid: ', len(X_valid), len(y_valid))\n",
    "\n",
    "history = model.fit_generator(\n",
    "    training_generator,\n",
    "    epochs = epochs, \n",
    "    validation_data = validation_data,\n",
    "    verbose = 1,\n",
    "    steps_per_epoch = X_train.shape[0] // batch_size,\n",
    "    callbacks=callbacks,\n",
    "    validation_steps = X_valid.shape[0] // batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.subplots(figsize=(12,8))\n",
    "plt.plot(history.history['accuracy'],\"r\")\n",
    "plt.plot(history.history['val_accuracy'],\"bo\")\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize=(12,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'],\"bo\")\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_valid = model.predict(X_valid, batch_size=128, verbose=1)\n",
    "# score = log_loss(Y_valid, predictions_valid)\n",
    "# print('Score log_loss: ', score)\n",
    "\n",
    "# # Store valid predictions\n",
    "# for i in range(len(test_index)):\n",
    "#     yfull_train[test_index[i]] = predictions_valid[i]\n",
    "\n",
    "# # Store test predictions\n",
    "# test_prediction = model.predict(test_data, batch_size=128, verbose=1)\n",
    "# yfull_test.append(test_prediction)\n",
    "\n",
    "# print('Final log_loss: {}, rows: {} cols: {} epoch: {}'.format(score, img_rows, img_cols, nb_epoch))\n",
    "# info_string = 'loss_' + str(score) \\\n",
    "#                 + '_r_' + str(img_rows) \\\n",
    "#                 + '_c_' + str(img_cols) \\\n",
    "#                 + '_ep_' + str(nb_epoch)\n",
    "\n",
    "# test_res = merge_several_folds_mean(yfull_test, 1)\n",
    "# create_submission(test_res, test_id, info_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
