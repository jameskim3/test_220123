{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import numbers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 42000\n",
      "Number of training pixels: 784\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('Data/train.csv')\n",
    "\n",
    "n_train = len(train_df)\n",
    "n_pixels = len(train_df.columns) - 1\n",
    "n_class = len(set(train_df['label']))\n",
    "\n",
    "print('Number of training samples: {0}'.format(n_train))\n",
    "print('Number of training pixels: {0}'.format(n_pixels))\n",
    "print('Number of classes: {0}'.format(n_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 28000\n",
      "Number of test pixels: 784\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "n_test = len(test_df)\n",
    "n_pixels = len(test_df.columns)\n",
    "\n",
    "print('Number of train samples: {0}'.format(n_test))\n",
    "print('Number of test pixels: {0}'.format(n_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAAFCCAYAAAD/pdQVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAZW0lEQVR4nO3de7TdZX3n8fcHIoigXDXDBGqwpS6ptIop4jhDD6CAYkVn6RocaoOlK62LaenUtRSdqUy9DDit4tRWnSxAQB0igzhSYaqpIaKtN4LKRbRErgE0agJiQTH4nT/2L/ZwcnIuYZ99yfN+rXVWzn5+z977s89K8jm/y352qgpJktSOXYYdQJIkDZblL0lSYyx/SZIaY/lLktQYy1+SpMZY/pIkNWbRsAMMygEHHFBLly4ddgxJkgZm3bp1P6iqp04db6b8ly5dynXXXTfsGJIkDUySO6cb97C/JEmNsfwlSWqM5S9JUmMsf0mSGmP5S5LUGMtfkqTGWP6SJDXG8pckqTGWvyRJjbH8JUlqjOUvSVJjmlnbv1VLz7pq2BG449yThh1BkjSJe/6SJDXG8pckqTGWvyRJjbH8JUlqjOUvSVJjLH9Jkhpj+UuS1BjLX5Kkxlj+kiQ1xvKXJKkxlr8kSY2x/CVJaozlL0lSYyx/SZIaY/lLktSYRcMOIEnaMUvPumrYEQC449yThh1B8+SevyRJjXHPX5qjUdjLcg9LUj+45y9JUmMsf0mSGmP5S5LUGMtfkqTGWP6SJDXG8pckqTGWvyRJjbH8JUlqjIv8SJKa19oiXpa/hm4U/tGBq+dJaoeH/SVJaox7/o/DKOyxurcqLQz/fWtn5p6/JEmNsfwlSWqM5S9JUmOGcs4/ya7AdcA9VfWyJIcAq4D9gOuB11bVI0l2By4Bngf8EPgPVXVH9xhvBk4HHgX+uKo+PfhXIo0Wz1NrFPn3cvQMa8//TOCWSbffBZxXVYcCm+mVOt2fm6vqV4DzunkkOQw4Bfg14ETg/d0vFJIkaRYDL/8kBwEnAed3twMcC1zeTbkYeEX3/cndbbrtx3XzTwZWVdVPq+p2YD1w5GBegSRJ420Ye/7vBd4I/Ly7vT9wf1Vt6W5vAJZ03y8B7gbotj/Qzf/F+DT3kSRJMxjoOf8kLwM2VtW6JBNbh6eZWrNsm+k+k59vBbACYPHixaxdu3a+kWf0hsO3zD5pgc32msw4d+OQc2fIOC7G4Wc5ChlhPHLuDBn7adAX/L0QeHmSlwJPBJ5C70jAPkkWdXv3BwH3dvM3AAcDG5IsAvYGNk0a32ryfX6hqlYCKwGWLVtWExMTfX0xp43CRSynTsy43YxzNw45d4aM42IcfpajkBHGI+fOkLGfBnrYv6reXFUHVdVSehfsramqU4FrgFd105YDn+y+v7K7Tbd9TVVVN35Kkt27dwocCnxlQC9DkqSxNirL+74JWJXkHcDXgAu68QuADydZT2+P/xSAqro5yWXAN4EtwBlV9ejgY0uSNH6GVv5VtRZY231/G9NcrV9VPwFevZ37vxN458IllCRp5+QKf5IkNcbylySpMZa/JEmNsfwlSWqM5S9JUmMsf0mSGjMq7/OX1BA/4lUaLvf8JUlqjOUvSVJjLH9Jkhpj+UuS1BjLX5Kkxlj+kiQ1xvKXJKkxlr8kSY2x/CVJaozlL0lSYyx/SZIaY/lLktQYy1+SpMZY/pIkNcbylySpMZa/JEmNsfwlSWqM5S9JUmMsf0mSGmP5S5LUGMtfkqTGWP6SJDXG8pckqTGWvyRJjbH8JUlqjOUvSVJjLH9Jkhpj+UuS1BjLX5Kkxlj+kiQ1xvKXJKkxlr8kSY2x/CVJaozlL0lSYyx/SZIaY/lLktQYy1+SpMZY/pIkNcbylySpMQMt/yRPTPKVJN9IcnOSP+/GD0ny5SS3JvlYkt268d272+u77UsnPdabu/FvJzlhkK9DkqRxNug9/58Cx1bVbwDPAU5MchTwLuC8qjoU2Ayc3s0/HdhcVb8CnNfNI8lhwCnArwEnAu9PsutAX4kkSWNqoOVfPT/ubj6h+yrgWODybvxi4BXd9yd3t+m2H5ck3fiqqvppVd0OrAeOHMBLkCRp7A38nH+SXZN8HdgIrAa+A9xfVVu6KRuAJd33S4C7AbrtDwD7Tx6f5j6SJGkGqarhPHGyD/AJ4K3Ah7pD+yQ5GLi6qg5PcjNwQlVt6LZ9h94e/tuAL1bVR7rxC7r7fHzKc6wAVgAsXrz4eatWrerra7jxngf6+ng74vAle8+43YxzNw45d4aMMB45zTh345BzZ8i4I4455ph1VbVs6viivj/THFXV/UnWAkcB+yRZ1O3dHwTc203bABwMbEiyCNgb2DRpfKvJ95n8HCuBlQDLli2riYmJvr6G0866qq+PtyPuOHVixu1mnLtxyLkzZITxyGnGuRuHnDtDxn4a9NX+T+32+EmyB/Ai4BbgGuBV3bTlwCe776/sbtNtX1O9QxVXAqd07wY4BDgU+MpgXoUkSeNt0Hv+BwIXd1fm7wJcVlWfSvJNYFWSdwBfAy7o5l8AfDjJenp7/KcAVNXNSS4DvglsAc6oqkcH/FokSRpLAy3/qroBeO4047cxzdX6VfUT4NXbeax3Au/sd0ZJknZ2rvAnSVJjLH9Jkhpj+UuS1Jg5l3+So5PstZ1teyU5un+xJEnSQpnPnv81wGHb2fbMbrskSRpx8yn/zLBtd8C32kmSNAZmfKtf9xG6z5g0tGyaQ/97AL8H3NXXZJIkaUHM9j7/5cDZ9D55r4D38dgjANXd3gKcsRABJUlSf81W/hcBa+kV/Bp6Bf/NKXN+CvxTVW3qdzhJktR/M5Z/Vd0J3AmQ5Bjg+qp6cBDBJEnSwpjz8r5V9bmFDCJJkgZjPu/z3y3J2Um+leShJI9O+dqykEElSVJ/zOeDff6C3jn//wdcQe9cvyRJGjPzKf9XAWd3n6YnSZLG1HwW+dkL+OJCBZEkSYMxn/L/W8D1+yVJGnPzOez/PuCSJD8Hrga2eV9/Vd3Wr2CSJGlhzKf8tx7y/2/0Vv2bzq6PK40kSVpw8yn/36O3nK8kSRpj81nk56IFzCFJkgZkPhf8SZKkncCc9/yTXDjLlKqq0x9nHkmStMDmc87/WLY9578f8GTg/u5LkiSNuPmc81863XiSo4EPAqf2KZMkSVpAj/ucf1VdC5xHbx0ASZI04vp1wd9twHP79FiSJGkBPe7yT7IIOA3Y8LjTSJKkBTefq/3XTDO8G/CrwP7AH/YrlCRJWjjzudp/F7a92v9B4ApgVVWt7VcoSZK0cOZztf/EAuaQJEkD4gp/kiQ1Zl7ln+TwJJcn+X6SLUk2JrksyeELFVCSJPXXfC74+03gc8DDwJXAd4F/Bfw2cFKSo6tq3YKklCRJfTOfC/7OAW4CjquqB7cOJnky8Pfd9uP7G0+SJPXbfA77HwWcM7n4Abrb7wJe0M9gkiRpYcyn/Ke+zW++2yVJ0giYT/l/GXhLd5j/F5LsCbwJ+FI/g0mSpIUxn3P+bwHWAncm+RRwH70L/k4CngT8Vt/TSZKkvpvPIj9fSXIU8FbgBGA/YBOwBnh7Vd24MBElSVI/zVj+SXaht2d/e1XdVFU3AK+aMudwYClg+UuSNAZmO+f/O8ClwD/PMOdB4NIkr+lbKkmStGDmUv4fqqrbtzehqu4ALgCW9zGXJElaILOV/xHAZ+bwOH8PLHv8cSRJ0kKbrfyfDGyew+Ns7uZKkqQRN1v5/wB4+hwe55e6uZIkacTNVv5fYG7n8k/r5kqSpBE3W/m/FzguyXlJdpu6MckTkvxP4FjgvNmeLMnBSa5JckuSm5Oc2Y3vl2R1klu7P/ftxpPkr5KsT3JDkiMmPdbybv6tSbzYUJKkOZrxff5V9cUkbwDeDZya5DPAnd3mpwMvBvYH3lBVc1ned0s39/pumeB1SVbTO3Lw2ao6N8lZwFn0lgx+CXBo9/V84APA85PsB5xN7yLD6h7nyqqay/UJkiQ1bdYV/qrqvUmup1fIrwT26DY9TG+533Or6vNzebKquo/essBU1YNJbgGWACcDE920i7vHfVM3fklVFfClJPskObCbu7qqNgF0v0CcSG9NAkmSNIM5Le9bVdcC13Yr/h3QDf+wqh7d0SdOshR4Lr0PDFrc/WJAVd2X5GndtCXA3ZPutqEb2964JEmaRXo71QN+0mQv4HPAO6vqiiT3V9U+k7Zvrqp9k1wFnFNVX+jGPwu8kd41BrtX1Tu68T8DHqqqd095nhXACoDFixc/b9WqVX19HTfe80BfH29HHL5k7xm3m3HuxiHnzpARxiOnGeduHHLuDBl3xDHHHLOuqrZZh2c+n+rXF0meAHwc+GhVXdENfy/Jgd1e/4HAxm58A3DwpLsfBNzbjU9MGV879bmqaiWwEmDZsmU1MTExdcrjctpZV/X18XbEHadOzLjdjHM3Djl3howwHjnNOHfjkHNnyNhPs13t31dJQm8p4Fuq6j2TNl3Jv7ylcDnwyUnjv9td9X8U8EB3euDTwPFJ9u3eGXB8NyZJkmYx6D3/FwKvBW5M8vVu7C3AucBlSU4H7gJe3W27GngpsB54CHgdQFVtSvJ24KvdvLdtvfhPkiTNbKDl3527z3Y2HzfN/ALO2M5jXQhc2L90kiS1YaCH/SVJ0vBZ/pIkNcbylySpMZa/JEmNsfwlSWqM5S9JUmMsf0mSGmP5S5LUGMtfkqTGWP6SJDXG8pckqTGWvyRJjbH8JUlqjOUvSVJjLH9Jkhpj+UuS1BjLX5Kkxlj+kiQ1xvKXJKkxlr8kSY2x/CVJaozlL0lSYyx/SZIaY/lLktQYy1+SpMZY/pIkNcbylySpMZa/JEmNsfwlSWqM5S9JUmMsf0mSGmP5S5LUGMtfkqTGWP6SJDXG8pckqTGWvyRJjbH8JUlqjOUvSVJjLH9Jkhpj+UuS1BjLX5Kkxlj+kiQ1xvKXJKkxlr8kSY2x/CVJaozlL0lSYyx/SZIaM9DyT3Jhko1Jbpo0tl+S1Ulu7f7ctxtPkr9Ksj7JDUmOmHSf5d38W5MsH+RrkCRp3A16z/8i4MQpY2cBn62qQ4HPdrcBXgIc2n2tAD4AvV8WgLOB5wNHAmdv/YVBkiTNbqDlX1XXApumDJ8MXNx9fzHwiknjl1TPl4B9khwInACsrqpNVbUZWM22v1BIkqTtGIVz/our6j6A7s+ndeNLgLsnzdvQjW1vXJIkzUGqarBPmCwFPlVVz+5u319V+0zavrmq9k1yFXBOVX2hG/8s8EbgWGD3qnpHN/5nwENV9e5pnmsFvVMGLF68+HmrVq3q62u58Z4H+vp4O+LwJXvPuN2MczcOOXeGjDAeOc04d+OQc2fIuCOOOeaYdVW1bOr4or4/0/x9L8mBVXVfd1h/Yze+ATh40ryDgHu78Ykp42une+CqWgmsBFi2bFlNTExMN22HnXbWVX19vB1xx6kTM24349yNQ86dISOMR04zzt045NwZMvbTKBz2vxLYesX+cuCTk8Z/t7vq/yjgge60wKeB45Ps213od3w3JkmS5mCge/5JLqW3135Akg30rto/F7gsyenAXcCru+lXAy8F1gMPAa8DqKpNSd4OfLWb97aqmnoRoSRJ2o6Bln9VvWY7m46bZm4BZ2zncS4ELuxjNEmSmjEKh/0lSdIAWf6SJDXG8pckqTGWvyRJjbH8JUlqjOUvSVJjLH9Jkhpj+UuS1BjLX5Kkxlj+kiQ1xvKXJKkxlr8kSY2x/CVJaozlL0lSYyx/SZIaY/lLktQYy1+SpMZY/pIkNcbylySpMZa/JEmNsfwlSWqM5S9JUmMsf0mSGmP5S5LUGMtfkqTGWP6SJDXG8pckqTGWvyRJjbH8JUlqjOUvSVJjLH9Jkhpj+UuS1BjLX5Kkxlj+kiQ1xvKXJKkxlr8kSY2x/CVJaozlL0lSYyx/SZIaY/lLktQYy1+SpMZY/pIkNcbylySpMZa/JEmNsfwlSWqM5S9JUmPGuvyTnJjk20nWJzlr2HkkSRoHY1v+SXYF/gZ4CXAY8Jokhw03lSRJo29syx84ElhfVbdV1SPAKuDkIWeSJGnkjXP5LwHunnR7QzcmSZJmkKoadoYdkuTVwAlV9fvd7dcCR1bVH02aswJY0d18JvDtgQed2QHAD4YdYg7GIacZ+2ccco5DRhiPnGbsn1HM+fSqeurUwUXDSNInG4CDJ90+CLh38oSqWgmsHGSo+UhyXVUtG3aO2YxDTjP2zzjkHIeMMB45zdg/45ITxvuw/1eBQ5MckmQ34BTgyiFnkiRp5I3tnn9VbUnyn4BPA7sCF1bVzUOOJUnSyBvb8geoqquBq4ed43EY2VMSU4xDTjP2zzjkHIeMMB45zdg/45JzfC/4kyRJO2acz/lLkqQdYPkPyTgsTZzkwiQbk9w07Czbk+TgJNckuSXJzUnOHHamqZI8MclXknyjy/jnw860PUl2TfK1JJ8adpbtSXJHkhuTfD3JdcPOM50k+yS5PMm3ur+bLxh2pqmSPLP7GW79+lGSPxl2rqmS/Ofu381NSS5N8sRhZ5oqyZldvptH8Wc4HQ/7D0G3NPE/AS+m95bFrwKvqapvDjXYFEmOBn4MXFJVzx52nukkORA4sKquT/JkYB3wilH6WSYJsGdV/TjJE4AvAGdW1ZeGHG0bSf4UWAY8papeNuw800lyB7Csqkbt/dS/kORi4PNVdX73bqQnVdX9w861Pd3/SfcAz6+qO4edZ6skS+j9ezmsqh5OchlwdVVdNNxk/yLJs+mtMHsk8Ajwd8Drq+rWoQabhXv+wzEWSxNX1bXApmHnmElV3VdV13ffPwjcwoit9Fg9P+5uPqH7GrnfupMcBJwEnD/sLOMsyVOAo4ELAKrqkVEu/s5xwHdGqfgnWQTskWQR8CSmrOcyAp4FfKmqHqqqLcDngFcOOdOsLP/hcGniBZBkKfBc4MvDTbKt7nD614GNwOqqGrmMwHuBNwI/H3aQWRTwmSTrulU8R80zgO8DH+pOoZyfZM9hh5rFKcClww4xVVXdA/wlcBdwH/BAVX1muKm2cRNwdJL9kzwJeCmPXYBuJFn+w5FpxkZuT3CcJNkL+DjwJ1X1o2HnmaqqHq2q59BbifLI7lDhyEjyMmBjVa0bdpY5eGFVHUHvEz3P6E5PjZJFwBHAB6rqucA/AyN5XQ9Ad1ri5cD/GXaWqZLsS++o6CHAvwb2TPI7w031WFV1C/AuYDW9Q/7fALYMNdQcWP7DMevSxJq77jz6x4GPVtUVw84zk+7w71rgxCFHmeqFwMu78+mrgGOTfGS4kaZXVfd2f24EPkHvNNoo2QBsmHR053J6vwyMqpcA11fV94YdZBovAm6vqu9X1c+AK4B/M+RM26iqC6rqiKo6mt6p0pE+3w+W/7C4NHGfdBfTXQDcUlXvGXae6SR5apJ9uu/3oPcf2reGm+qxqurNVXVQVS2l9/dxTVWN1B4WQJI9uws76Q6lH0/vsOvIqKrvAncneWY3dBwwMhegTuM1jOAh/85dwFFJntT9Wz+O3nU9IyXJ07o/fwn494zuz/MXxnqFv3E1LksTJ7kUmAAOSLIBOLuqLhhuqm28EHgtcGN3Th3gLd3qj6PiQODi7orqXYDLqmpk30o34hYDn+j1AIuA/11VfzfcSNP6I+Cj3S/3twGvG3KeaXXnqF8M/MGws0ynqr6c5HLgenqH0r/GaK6i9/Ek+wM/A86oqs3DDjQb3+onSVJjPOwvSVJjLH9Jkhpj+UuS1BjLX5Kkxlj+kiQ1xvKXtF1JXpDksiT3JnkkyQ+TrE6yvFuy+LQk1S2tLGlM+D5/SdPqPpr0PcAa4E3AncC+9BbW+QAw6h9WI2k7fJ+/pG106+WvBf66qv54mu2/DOxJb9naDwGHVNUdg8woacd52F/SdM6it0b5G6fbWFXfqaobptuW5JQka5J8P8mPu0+2Wz7NvDOT3JLk4SSbk1yX5JWTtp+Q5B+SPNA9zreTvLVfL1BqmYf9JT1GtwzxBPB/q+onO/AQz6D3YTbn0vt44KOB85PsUVUf7J7jVODdwNuAzwN7AL8O7Ndtfwa9z7u4HHg78AhwaPfYkh4ny1/SVAfQK+M7d+TOVfXft36fZBd6pw8OBF4PfLDb9ALghqp626S7Tv48hiOA3YDXT/qI5jU7kkfStjzsL6mvkhya5NIk99D7oJOfAb8PPHPStK8Cz0nyviQv6j5gZrKvd/dbleRVWz81TVJ/WP6Spvoh8DDw9PneMclewGrgN+hdN/DvgN8ELgR2nzT1EnpHAp5P79MtNyW5YutbBqtqPXACvf+jPgx8N8mXk/zWjr0kSZNZ/pIeo6q20DtU/+Iku88yfaoX0PulYUVVfbiq/rGqrmPKKcbq+V9VdSS90wzLgSOBj02ac01VnQjsA7yI3pGAq5IcsIMvTVLH8pc0nXOB/YG/mG5jkkOS/Po0m7Yevv/ZpLn7Aidv74mqanNVfQy4DHj2NNt/WlVrgP9B7+2Fh8z1RUianhf8SdpGVV2b5E+B9yR5FnARcBe9RX6Oo3cO/z9Oc9d/BH4E/E2Ss+mV9X8FfgDsvXVSkpXAg8AXgY3ArwKvBT7Tbf9Deu8SuBq4m97RgTcD9wI39ffVSu1xz1/StKrqvcC/pbeS31/Su9r+IuBZwB8AfzvNfb4PvBLYld7b9M4Bzgc+MmXqPwDPA95P7xqB/9LN2boewDfo/eJwDr1fCP4auB04tqoe7tNLlJrlCn+SJDXGPX9Jkhpj+UuS1BjLX5Kkxlj+kiQ1xvKXJKkxlr8kSY2x/CVJaozlL0lSYyx/SZIa8/8Bx1RKFijddX0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "plt.bar(train_df['label'].value_counts().index, train_df['label'].value_counts())\n",
    "plt.xticks(np.arange(n_class))\n",
    "plt.xlabel('Class', fontsize=16)\n",
    "plt.ylabel('Count', fontsize=16)\n",
    "plt.grid('on', axis='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_data(Dataset):\n",
    "    \"\"\"MNIST dtaa set\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path, \n",
    "                 transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), \n",
    "                     transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "                ):\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if len(df.columns) == n_pixels:\n",
    "            # test data\n",
    "            self.X = df.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n",
    "            self.y = None\n",
    "        else:\n",
    "            # training data\n",
    "            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n",
    "            self.y = torch.from_numpy(df.iloc[:,0].values)\n",
    "            \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.transform(self.X[idx]), self.y[idx]\n",
    "        else:\n",
    "            return self.transform(self.X[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random rotation transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation(object):\n",
    "    \"\"\"\n",
    "    https://github.com/pytorch/vision/tree/master/torchvision/transforms\n",
    "    Rotate the image by angle.\n",
    "    Args:\n",
    "        degrees (sequence or float or int): Range of degrees to select from.\n",
    "            If degrees is a number instead of sequence like (min, max), the range of degrees\n",
    "            will be (-degrees, +degrees).\n",
    "        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n",
    "            An optional resampling filter.\n",
    "            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n",
    "            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n",
    "        expand (bool, optional): Optional expansion flag.\n",
    "            If true, expands the output to make it large enough to hold the entire rotated image.\n",
    "            If false or omitted, make the output image the same size as the input image.\n",
    "            Note that the expand flag assumes rotation around the center and no translation.\n",
    "        center (2-tuple, optional): Optional center of rotation.\n",
    "            Origin is the upper left corner.\n",
    "            Default is the center of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees, resample=False, expand=False, center=None):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError(\"If degrees is a single number, it must be positive.\")\n",
    "            self.degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            if len(degrees) != 2:\n",
    "                raise ValueError(\"If degrees is a sequence, it must be of len 2.\")\n",
    "            self.degrees = degrees\n",
    "\n",
    "        self.resample = resample\n",
    "        self.expand = expand\n",
    "        self.center = center\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(degrees):\n",
    "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
    "        Returns:\n",
    "            sequence: params to be passed to ``rotate`` for random rotation.\n",
    "        \"\"\"\n",
    "        angle = np.random.uniform(degrees[0], degrees[1])\n",
    "\n",
    "        return angle\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "            img (PIL Image): Image to be rotated.\n",
    "        Returns:\n",
    "            PIL Image: Rotated image.\n",
    "        \"\"\"\n",
    "        \n",
    "        def rotate(img, angle, resample=False, expand=False, center=None):\n",
    "            \"\"\"Rotate the image by angle and then (optionally) translate it by (n_columns, n_rows)\n",
    "            Args:\n",
    "            img (PIL Image): PIL Image to be rotated.\n",
    "            angle ({float, int}): In degrees degrees counter clockwise order.\n",
    "            resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n",
    "            An optional resampling filter.\n",
    "            See http://pillow.readthedocs.io/en/3.4.x/handbook/concepts.html#filters\n",
    "            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n",
    "            expand (bool, optional): Optional expansion flag.\n",
    "            If true, expands the output image to make it large enough to hold the entire rotated image.\n",
    "            If false or omitted, make the output image the same size as the input image.\n",
    "            Note that the expand flag assumes rotation around the center and no translation.\n",
    "            center (2-tuple, optional): Optional center of rotation.\n",
    "            Origin is the upper left corner.\n",
    "            Default is the center of the image.\n",
    "            \"\"\"\n",
    "                \n",
    "            return img.rotate(angle, resample, expand, center)\n",
    "\n",
    "        angle = self.get_params(self.degrees)\n",
    "\n",
    "        return rotate(img, angle, self.resample, self.expand, self.center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random vertical and horizontal shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomShift(object):\n",
    "    def __init__(self, shift):\n",
    "        self.shift = shift\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_params(shift):\n",
    "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
    "        Returns:\n",
    "            sequence: params to be passed to ``rotate`` for random rotation.\n",
    "        \"\"\"\n",
    "        hshift, vshift = np.random.uniform(-shift, shift, size=2)\n",
    "\n",
    "        return hshift, vshift \n",
    "    def __call__(self, img):\n",
    "        hshift, vshift = self.get_params(self.shift)\n",
    "        \n",
    "        return img.transform(img.size, Image.AFFINE, (1,0,hshift,0,1,vshift), resample=Image.BICUBIC, fill=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data into tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = MNIST_data('data/train.csv', transform= transforms.Compose(\n",
    "                            [transforms.ToPILImage(), RandomRotation(degrees=20), RandomShift(3),\n",
    "                             transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))]))\n",
    "test_dataset = MNIST_data('data/test.csv')\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b1b5dcc3be2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Apply each of the above transforms on sample.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m65\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsfrm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrotate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtransformed_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsfrm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5272\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5273\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5276\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rotate = RandomRotation(20)\n",
    "shift = RandomShift(3)\n",
    "composed = transforms.Compose([RandomRotation(20),\n",
    "                               RandomShift(3)])\n",
    "\n",
    "# Apply each of the above transforms on sample.\n",
    "fig = plt.figure()\n",
    "sample = transforms.ToPILImage()(train_df.iloc[65,1:].reshape((28,28)).astype(np.uint8)[:,:,None])\n",
    "for i, tsfrm in enumerate([rotate, shift, composed]):\n",
    "    transformed_sample = tsfrm(sample)\n",
    "\n",
    "    ax = plt.subplot(1, 3, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(type(tsfrm).__name__)\n",
    "    ax.imshow(np.reshape(np.array(list(transformed_sample.getdata())), (-1,28)), cmap='gray')    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "          \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "          \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p = 0.5),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "          \n",
    "        for m in self.features.children():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "        for m in self.classifier.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipofri-desktop\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    exp_lr_scheduler.step()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx + 1)% 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in data_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    loss /= len(data_loader.dataset)\n",
    "        \n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [6400/42000 (15%)]\tLoss: 0.030782\n",
      "Train Epoch: 0 [12800/42000 (30%)]\tLoss: 0.047934\n",
      "Train Epoch: 0 [19200/42000 (46%)]\tLoss: 0.145169\n",
      "Train Epoch: 0 [25600/42000 (61%)]\tLoss: 0.087086\n",
      "Train Epoch: 0 [32000/42000 (76%)]\tLoss: 0.093110\n",
      "Train Epoch: 0 [38400/42000 (91%)]\tLoss: 0.049615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ipofri-desktop\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 0.0588, Accuracy: 41217/42000 (98.136%)\n",
      "\n",
      "Train Epoch: 1 [6400/42000 (15%)]\tLoss: 0.023840\n",
      "Train Epoch: 1 [12800/42000 (30%)]\tLoss: 0.198949\n",
      "Train Epoch: 1 [19200/42000 (46%)]\tLoss: 0.021373\n",
      "Train Epoch: 1 [25600/42000 (61%)]\tLoss: 0.089959\n",
      "Train Epoch: 1 [32000/42000 (76%)]\tLoss: 0.213689\n",
      "Train Epoch: 1 [38400/42000 (91%)]\tLoss: 0.189057\n",
      "\n",
      "Average loss: 0.0613, Accuracy: 41209/42000 (98.117%)\n",
      "\n",
      "Train Epoch: 2 [6400/42000 (15%)]\tLoss: 0.093854\n",
      "Train Epoch: 2 [12800/42000 (30%)]\tLoss: 0.017098\n",
      "Train Epoch: 2 [19200/42000 (46%)]\tLoss: 0.057864\n",
      "Train Epoch: 2 [25600/42000 (61%)]\tLoss: 0.094945\n",
      "Train Epoch: 2 [32000/42000 (76%)]\tLoss: 0.030812\n",
      "Train Epoch: 2 [38400/42000 (91%)]\tLoss: 0.014774\n",
      "\n",
      "Average loss: 0.0527, Accuracy: 41330/42000 (98.405%)\n",
      "\n",
      "Train Epoch: 3 [6400/42000 (15%)]\tLoss: 0.112518\n",
      "Train Epoch: 3 [12800/42000 (30%)]\tLoss: 0.101271\n",
      "Train Epoch: 3 [19200/42000 (46%)]\tLoss: 0.122425\n",
      "Train Epoch: 3 [25600/42000 (61%)]\tLoss: 0.183062\n",
      "Train Epoch: 3 [32000/42000 (76%)]\tLoss: 0.091614\n",
      "Train Epoch: 3 [38400/42000 (91%)]\tLoss: 0.028191\n",
      "\n",
      "Average loss: 0.0335, Accuracy: 41576/42000 (98.990%)\n",
      "\n",
      "Train Epoch: 4 [6400/42000 (15%)]\tLoss: 0.023565\n",
      "Train Epoch: 4 [12800/42000 (30%)]\tLoss: 0.181720\n",
      "Train Epoch: 4 [19200/42000 (46%)]\tLoss: 0.006581\n",
      "Train Epoch: 4 [25600/42000 (61%)]\tLoss: 0.064543\n",
      "Train Epoch: 4 [32000/42000 (76%)]\tLoss: 0.067131\n",
      "Train Epoch: 4 [38400/42000 (91%)]\tLoss: 0.129310\n",
      "\n",
      "Average loss: 0.0311, Accuracy: 41573/42000 (98.983%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "    evaluate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
